{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_vocab_list(data_set):\n",
    "    \"\"\"\n",
    "    返回不重复词的列表\n",
    "    \"\"\"\n",
    "    vocab_set = set([])\n",
    "    for document in data_set:\n",
    "        vocab_set = vocab_set | set(document)  # 求并集\n",
    "    return list(vocab_set)\n",
    "\n",
    "\n",
    "def set_of_words2vec(vocab_list, input_set):\n",
    "    \"\"\"\n",
    "    将文档转换为词向量（词集模型）\n",
    "    \"\"\"\n",
    "    return_vec = [0]*len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vec[vocab_list.index(word)] = 1\n",
    "        # else:\n",
    "            # print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return return_vec\n",
    "\n",
    "def bag_of_words2vec(vocab_list, input_set):\n",
    "    \"\"\"\n",
    "    将文档转换为词向量（词袋模型）\n",
    "    \"\"\"\n",
    "    return_vec = [0]*len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vec[vocab_list.index(word)] += 1\n",
    "        # else:\n",
    "            # print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return return_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data_set():\n",
    "    \"\"\"\n",
    "    示例数据\n",
    "    \"\"\"\n",
    "    posting_list = [\n",
    "        ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "        ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "        ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "        ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "        ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "        ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
    "    ]\n",
    "    class_vec = [0, 1, 0, 1, 0, 1]  # 1 代表侮辱性文字；0代表正常言论\n",
    "    return posting_list, class_vec\n",
    "\n",
    "list_posts, list_classes = load_data_set()  # 示例评论，示例评论所属分类\n",
    "my_vocab_list = create_vocab_list(list_posts)  # 词库\n",
    "set_of_words2vec(my_vocab_list, list_posts[0])  # 词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n求某个文档d是某个类别c的概率：\\np(c|d) = p(d|c)p(c) / p(d)\\n\\n文档d可以表示为词向量(w1,w2,...,wn)，p(d|c) = p(w1,w2,...,wn|c)，\\n朴素贝叶斯假设所有词均独立，即p(d|c) = p(w1,w2,...,wn|c)=p(w1|c)p(w2|c)...p(wn|c)\\n\\n根据训练样本可以求p(c)、p(w1|c)、p(w2|c)、p(wn|c)\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "求某个文档d是某个类别c的概率：\n",
    "p(c|d) = p(d|c)p(c) / p(d)\n",
    "\n",
    "文档d可以表示为词向量(w1,w2,...,wn)，p(d|c) = p(w1,w2,...,wn|c)，\n",
    "朴素贝叶斯假设所有词均独立，即p(d|c) = p(w1,w2,...,wn|c)=p(w1|c)p(w2|c)...p(wn|c)\n",
    "\n",
    "根据训练样本可以求p(c)、p(w1|c)、p(w2|c)、p(wn|c)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_nb0(train_matrix, train_category):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类器训练函数\n",
    "    \n",
    "    Parameters\n",
    "        train_matrix：由词向量组成的list\n",
    "        train_category：List，每个词向量的类别标签\n",
    "        \n",
    "    Return\n",
    "        p0_vect：每个词属于类别0（正常言论）的概率\n",
    "        p1_vect：每个词属于类别1（侮辱性言论）的概率\n",
    "        p_abusive：文档属于侮辱性文档的概率\n",
    "    \"\"\"\n",
    "    num_train_docs = len(train_matrix)  # 文档数量\n",
    "    num_words = len(train_matrix[0])  # 词向量长度\n",
    "    p_abusive = sum(train_category) / float(num_train_docs)  # 文档属于侮辱性文档的概率\n",
    "    # p0_num = np.zeros(num_words)  # 每个词在正常言论中出现的次数\n",
    "    # p1_num = np.zeros(num_words)  # 每个词在侮辱性言论中出现的次数\n",
    "    p0_num = np.ones(num_words)  # 用ones代替zeros，降低概率为0导致乘积为0带来的影响\n",
    "    p1_num = np.ones(num_words)\n",
    "    # p0_denom = 0.0  # 正常言论中所有单词出现次数的总和\n",
    "    # p1_denom = 0.0  # 侮辱性言中所有单词出现次数的总和\n",
    "    p0_denom = 2.0  # 用2代替0\n",
    "    p1_denom = 2.0\n",
    "\n",
    "    for i in range(num_train_docs):\n",
    "        if train_category[i] == 1:\n",
    "            p1_num += train_matrix[i]\n",
    "            p1_denom += sum(train_matrix[i])\n",
    "        else:\n",
    "            p0_num += train_matrix[i]\n",
    "            p0_denom += sum(train_matrix[i])\n",
    "\n",
    "    # p1_vect = p1_num / p1_denom\n",
    "    # p0_vect = p0_num / p0_denom\n",
    "    p1_vect = np.log(p1_num / p1_denom)  # 取对数。当概率很小时相乘会很小，甚至接近于0，取对数可缓解这种情况\n",
    "    p0_vect = np.log(p0_num / p0_denom)\n",
    "\n",
    "    return p0_vect, p1_vect, p_abusive\n",
    "\n",
    "# 获得文档的词向量\n",
    "train_mat = []\n",
    "for post_in_doc in list_posts:\n",
    "    train_mat.append(set_of_words2vec(my_vocab_list, post_in_doc))\n",
    "\n",
    "# 训练。求每个词属于哪个类别的概率\n",
    "p0_v, p1_v, p_ab = train_nb0(train_mat, list_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[-2.56494936 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -3.25809654 -2.56494936 -3.25809654 -1.87180218 -2.56494936 -2.56494936\n",
      " -2.56494936 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -2.56494936 -3.25809654 -3.25809654 -3.25809654 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654\n",
      " -3.25809654 -2.15948425]\n",
      "[-2.35137526 -3.04452244 -3.04452244 -3.04452244 -2.35137526 -3.04452244\n",
      " -2.35137526 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -3.04452244\n",
      " -3.04452244 -2.35137526 -1.94591015 -3.04452244 -1.94591015 -3.04452244\n",
      " -3.04452244 -1.65822808 -2.35137526 -2.35137526 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244 -2.35137526 -3.04452244 -3.04452244 -2.35137526\n",
      " -2.35137526 -2.35137526]\n",
      "['stop', 'help', 'has', 'how', 'buying', 'so', 'garbage', 'dalmation', 'maybe', 'my', 'I', 'love', 'steak', 'to', 'dog', 'flea', 'worthless', 'licks', 'mr', 'stupid', 'posting', 'quit', 'food', 'please', 'is', 'problems', 'park', 'ate', 'cute', 'take', 'not', 'him']\n"
     ]
    }
   ],
   "source": [
    "# 查看训练器\n",
    "print(p_ab) # 侮辱性言论的概率\n",
    "print(p0_v) # 每个词是正常言论的概率\n",
    "print(p1_v) # 每个词是侮辱性言论的概率\n",
    "print(my_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as : 0\n",
      "['stupid', 'garbage'] classified as : 1\n"
     ]
    }
   ],
   "source": [
    "def classify_nb(vec2classify, p0_vec, p1_vec, p_class1):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类函数\n",
    "    \n",
    "    Parameters\n",
    "        vec2classify：需要分类的词向量\n",
    "        p0_vec：每个词属于类别0（正常言论）的概率\n",
    "        p1_vect：每个词属于类别1（侮辱性言论）的概率\n",
    "        p_class1：文档属于侮辱性文档的概率\n",
    "    \"\"\"\n",
    "    p1 = sum(vec2classify * p1_vec) + np.log(p_class1)  # log(ab) = log(a) + log(b)\n",
    "    p0 = sum(vec2classify * p0_vec) + np.log(1.0 - p_class1)\n",
    "\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "test_list = [\n",
    "    ['love', 'my', 'dalmation'],\n",
    "    ['stupid', 'garbage']\n",
    "]\n",
    "\n",
    "# 利用训练器分类\n",
    "for test_doc in test_list:\n",
    "    test_class = classify_nb(set_of_words2vec(my_vocab_list, test_doc), p0_v, p1_v, p_ab)\n",
    "    print(\"%s classified as : %s\" % (test_doc, test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.0\n"
     ]
    }
   ],
   "source": [
    "# 示例：垃圾邮件分类\n",
    "\n",
    "def text_parse(big_string):\n",
    "    \"\"\"\n",
    "    将文本分割为list\n",
    "    \"\"\"\n",
    "    list_of_tokens = re.split(r'\\W+', big_string)\n",
    "    return [tok.lower() for tok in list_of_tokens if len(tok) > 2]\n",
    "\n",
    "def spam_test():\n",
    "    \"\"\"\n",
    "    垃圾邮件测试\n",
    "    \"\"\"\n",
    "    \n",
    "    # 读取邮件内容\n",
    "    doc_list = []; class_list = []; full_text = []\n",
    "    for i in range(1,26):\n",
    "        word_list = text_parse(open('data/email/spam/%s.txt' % i,encoding='utf-8').read())\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(1)\n",
    "        word_list = text_parse(open('data/email/ham/%d.txt' % i,encoding='utf-8').read())\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(0)\n",
    "    \n",
    "    # 词库\n",
    "    vocab_list = create_vocab_list(doc_list)\n",
    "    \n",
    "    # 构造训练集、测试集\n",
    "    training_set = list(range(50)); test_set = []\n",
    "    for i in range(10):\n",
    "        rand_index = int(np.random.uniform(0, len(training_set)))\n",
    "        test_set.append(training_set[rand_index])\n",
    "        del training_set[rand_index]\n",
    "\n",
    "    # 训练模型\n",
    "    train_mat = []; train_classes = []\n",
    "    for doc_index in training_set:\n",
    "        train_mat.append(set_of_words2vec(vocab_list, doc_list[doc_index]))\n",
    "        train_classes.append(class_list[doc_index])\n",
    "    p0_v, p1_v, p_spam = train_nb0(np.array(train_mat), np.array(train_classes))\n",
    "\n",
    "    # 测试\n",
    "    error_count = 0\n",
    "    for doc_index in test_set:\n",
    "        word_vector = set_of_words2vec(vocab_list, doc_list[doc_index])\n",
    "        if classify_nb(np.array(word_vector), p0_v, p1_v, p_spam) != class_list[doc_index]:\n",
    "            error_count += 1\n",
    "    print(\"the error rate is: \", float(error_count) / len(test_set))\n",
    "\n",
    "spam_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 示例：发现地域相关的用词\n",
    "\n",
    "import feedparser\n",
    "\n",
    "ny=feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n",
    "sf=feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 示例：发现地域相关的用词\n",
    "\n",
    "def clac_most_freq(vocab_list, full_text):\n",
    "    \"\"\"\n",
    "    计算词频，取出现次数多的词\n",
    "    \"\"\"\n",
    "    import operator\n",
    "    freq_dict = {}\n",
    "    for token in vocab_list:\n",
    "        freq_dict[token] = full_text.count(token)\n",
    "    sorted_freq = sorted(freq_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sorted_freq[:30]\n",
    "\n",
    "\n",
    "def local_words(feed1, feed0):\n",
    "    # 读取rss内容\n",
    "    doc_list = []; class_list = []; full_text = []\n",
    "    min_len = min(len(feed1['entries']), len(feed0['entries']))\n",
    "    for i in range(min_len):\n",
    "        word_list = text_parse(feed1['entries'][i]['summary'])\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(1)\n",
    "        word_list = text_parse(feed0['entries'][i]['summary'])\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(0)\n",
    "    \n",
    "    # 词库\n",
    "    vocal_list = create_vocab_list(doc_list)\n",
    "    \n",
    "    # 取出现次数多的词\n",
    "    top30_words = clac_most_freq(vocal_list, full_text)\n",
    "    \n",
    "    # 移除高频词\n",
    "    for pair_w in top30_words:\n",
    "        if pair_w[0] in vocal_list:\n",
    "            vocal_list.remove(pair_w[0])\n",
    "    \n",
    "    # 构建训练集和测试集\n",
    "    training_set = list(range(2*min_len)); test_set = []\n",
    "    for i in range(20):\n",
    "        rand_indx = int(np.random.uniform(0, len(training_set)))\n",
    "        test_set.append(training_set[rand_indx])\n",
    "        del(training_set[rand_indx])\n",
    "\n",
    "    # 训练模型\n",
    "    train_mat = []; train_classes = []\n",
    "    for doc_index in training_set:\n",
    "        train_mat.append(bag_of_words2vec(vocal_list, doc_list[doc_index]))\n",
    "        train_classes.append(class_list[doc_index])\n",
    "    p0_v, p1_v, p_spam = train_nb0(np.array(train_mat), np.array(train_classes))\n",
    "\n",
    "    # 测试\n",
    "    error_count = 0\n",
    "    for doc_index in test_set:\n",
    "        word_vector = bag_of_words2vec(vocal_list, doc_list[doc_index])\n",
    "        if classify_nb(np.array(word_vector), p0_v, p1_v, p_spam) != class_list[doc_index]:\n",
    "            error_count += 1\n",
    "    print('the error rate is: ', float(error_count)/len(test_set))\n",
    "    return vocal_list, p0_v, p1_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.35\n",
      "SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\n",
      "find\n",
      "things\n",
      "about\n",
      "area\n",
      "meet\n",
      "here\n",
      "just\n",
      "married\n",
      "get\n",
      "friendship\n",
      "prefer\n",
      "now\n",
      "what\n",
      "happy\n",
      "relationship\n",
      "forward\n",
      "being\n",
      "new\n",
      "employed\n",
      "little\n",
      "enjoy\n",
      "all\n",
      "tamil\n",
      "wish\n",
      "know\n",
      "daddy\n",
      "hang\n",
      "helper\n",
      "love\n",
      "young\n",
      "mind\n",
      "email\n",
      "texting\n",
      "open\n",
      "horny\n",
      "than\n",
      "there\n",
      "lingerie\n",
      "seeking\n",
      "years\n",
      "believes\n",
      "are\n",
      "click\n",
      "bay\n",
      "ever\n",
      "don\n",
      "NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\n",
      "host\n",
      "that\n",
      "come\n",
      "place\n",
      "massage\n",
      "best\n",
      "handsome\n",
      "day\n",
      "9832\n",
      "nice\n",
      "now\n",
      "over\n",
      "stress\n",
      "from\n",
      "cool\n",
      "beautiful\n",
      "reply\n",
      "available\n",
      "9745\n",
      "professional\n",
      "relaxing\n",
      "want\n",
      "also\n",
      "travel\n",
      "pic\n",
      "420\n",
      "full\n",
      "candles\n",
      "give\n",
      "seaching\n",
      "wants\n",
      "buddy\n",
      "two\n",
      "nothing\n",
      "gentlemen\n",
      "waiting\n",
      "let\n",
      "great\n",
      "fun\n",
      "weed\n",
      "sitting\n",
      "here\n",
      "panties\n",
      "awesome\n",
      "secret\n",
      "girl\n",
      "music\n",
      "mature\n",
      "four\n",
      "details\n",
      "five\n",
      "guys\n",
      "make\n",
      "about\n",
      "built\n",
      "got\n",
      "three\n",
      "one\n",
      "technique\n",
      "lesbian\n",
      "kept\n",
      "female\n",
      "body\n",
      "scented\n",
      "lotions\n",
      "list\n",
      "smoke\n",
      "oral\n",
      "ready\n"
     ]
    }
   ],
   "source": [
    "def get_top_words(ny, sf):\n",
    "    \"\"\"\n",
    "    最具表征性的词汇显示函数\n",
    "    \"\"\"\n",
    "    import operator\n",
    "    vovab_list, p0_v, p1_v = local_words(ny, sf)\n",
    "    top_ny = []; top_sf = []\n",
    "    for i in range(len(p0_v)):\n",
    "        if p0_v[i] > -5 : top_sf.append((vovab_list[i], p0_v[i]))\n",
    "        if p1_v[i] > -5 : top_ny.append((vovab_list[i], p1_v[i]))\n",
    "\n",
    "    sorted_sf = sorted(top_sf, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\")\n",
    "    for item in sorted_sf:\n",
    "        print(item[0])\n",
    "\n",
    "    sorted_ny = sorted(top_ny, key=lambda pair: pair[1], reverse=True)\n",
    "    print(\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\")\n",
    "    for item in sorted_ny:\n",
    "        print(item[0])\n",
    "        \n",
    "get_top_words(ny,sf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
