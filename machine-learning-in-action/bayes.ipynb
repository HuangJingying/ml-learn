{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data_set():\n",
    "    posting_list = [\n",
    "        ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "        ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "        ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "        ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "        ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "        ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
    "    ]\n",
    "    class_vec = [0, 1, 0, 1, 0, 1]  # 1 代表侮辱性文字；0代表正常言论\n",
    "    return posting_list, class_vec\n",
    "\n",
    "\n",
    "def create_vocab_list(data_set):\n",
    "    \"\"\"\n",
    "    返回不重复词的列表\n",
    "    \"\"\"\n",
    "    vocab_set = set([])\n",
    "    for document in data_set:\n",
    "        vocab_set = vocab_set | set(document)  # 求并集\n",
    "    return list(vocab_set)\n",
    "\n",
    "\n",
    "def set_of_words2vec(vocab_list, input_set):\n",
    "    \"\"\"\n",
    "    将文档转换为词向量\n",
    "    \"\"\"\n",
    "    return_vec = [0]*len(vocab_list)\n",
    "    for word in input_set:\n",
    "        if word in vocab_list:\n",
    "            return_vec[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return return_vec\n",
    "\n",
    "list_posts, list_classes = load_data_set()\n",
    "my_vocab_list = create_vocab_list(list_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_of_words2vec(my_vocab_list, list_posts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n求某个文档d是某个类别c的概率：\\np(c|d) = p(d|c)p(c) / p(d)\\n\\n文档d可以表示为词向量(w1,w2,...,wn)，p(d|c) = p(w1,w2,...,wn|c)，\\n朴素贝叶斯假设所有词均独立，即p(d|c) = p(w1,w2,...,wn|c)=p(w1|c)p(w2|c)...p(wn|c)\\n\\n根据训练样本可以求p(c)、p(w1|c)、p(w2|c)、p(wn|c)\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "求某个文档d是某个类别c的概率：\n",
    "p(c|d) = p(d|c)p(c) / p(d)\n",
    "\n",
    "文档d可以表示为词向量(w1,w2,...,wn)，p(d|c) = p(w1,w2,...,wn|c)，\n",
    "朴素贝叶斯假设所有词均独立，即p(d|c) = p(w1,w2,...,wn|c)=p(w1|c)p(w2|c)...p(wn|c)\n",
    "\n",
    "根据训练样本可以求p(c)、p(w1|c)、p(w2|c)、p(wn|c)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_nb0(train_matrix, train_category):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类器训练函数\n",
    "    \n",
    "    Parameters\n",
    "        train_matrix：由词向量组成的list\n",
    "        train_category：List，每个词向量的类别标签\n",
    "        \n",
    "    Return\n",
    "        p0_vect：每个词属于类别0（正常言论）的概率\n",
    "        p1_vect：每个词属于类别1（侮辱性言论）的概率\n",
    "        p_abusive：文档属于侮辱性文档的概率\n",
    "    \"\"\"\n",
    "    num_train_docs = len(train_matrix)  # 文档数量\n",
    "    num_words = len(train_matrix[0])  # 词向量长度\n",
    "    p_abusive = sum(train_category) / float(num_train_docs)  # 文档属于侮辱性文档的概率\n",
    "    # p0_num = np.zeros(num_words)  # 每个词在正常言论中出现的次数\n",
    "    # p1_num = np.zeros(num_words)  # 每个词在侮辱性言论中出现的次数\n",
    "    p0_num = np.ones(num_words)  # 用ones代替zeros，降低概率为0导致乘积为0带来的影响\n",
    "    p1_num = np.ones(num_words)\n",
    "    # p0_denom = 0.0  # 正常言论中所有单词出现次数的总和\n",
    "    # p1_denom = 0.0  # 侮辱性言中所有单词出现次数的总和\n",
    "    p0_denom = 2.0  # 用2代替0\n",
    "    p1_denom = 2.0\n",
    "\n",
    "    for i in range(num_train_docs):\n",
    "        if train_category[i] == 1:\n",
    "            p1_num += train_matrix[i]\n",
    "            p1_denom += sum(train_matrix[i])\n",
    "        else:\n",
    "            p0_num += train_matrix[i]\n",
    "            p0_denom += sum(train_matrix[i])\n",
    "\n",
    "    # p1_vect = p1_num / p1_denom\n",
    "    # p0_vect = p0_num / p0_denom\n",
    "    p1_vect = np.log(p1_num / p1_denom)  # 取对数。当概率很小时相乘会很小，甚至接近于0，取对数可缓解这种情况\n",
    "    p0_vect = np.log(p0_num / p0_denom)\n",
    "\n",
    "    return p0_vect, p1_vect, p_abusive\n",
    "\n",
    "# 获得文档的词向量\n",
    "train_mat = []\n",
    "for post_in_doc in list_posts:\n",
    "    train_mat.append(set_of_words2vec(my_vocab_list, post_in_doc))\n",
    "\n",
    "# 求每个词属于哪个类别的概率\n",
    "p0_v, p1_v, p_ab = train_nb0(train_mat, list_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "[-1.87180218 -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936\n",
      " -2.56494936 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
      " -2.56494936 -2.56494936 -2.15948425 -3.25809654 -3.25809654 -2.56494936\n",
      " -3.25809654 -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936\n",
      " -3.25809654 -2.56494936 -2.56494936 -3.25809654 -3.25809654 -2.56494936\n",
      " -3.25809654 -2.56494936]\n",
      "[-3.04452244 -3.04452244 -3.04452244 -2.35137526 -3.04452244 -3.04452244\n",
      " -3.04452244 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -3.04452244\n",
      " -3.04452244 -3.04452244 -2.35137526 -2.35137526 -1.65822808 -1.94591015\n",
      " -2.35137526 -2.35137526 -3.04452244 -3.04452244 -1.94591015 -3.04452244\n",
      " -2.35137526 -3.04452244 -3.04452244 -2.35137526 -2.35137526 -2.35137526\n",
      " -2.35137526 -3.04452244]\n",
      "['my', 'mr', 'ate', 'posting', 'is', 'problems', 'dalmation', 'I', 'so', 'garbage', 'not', 'steak', 'licks', 'how', 'him', 'park', 'stupid', 'dog', 'buying', 'to', 'help', 'please', 'worthless', 'love', 'quit', 'has', 'flea', 'take', 'food', 'stop', 'maybe', 'cute']\n"
     ]
    }
   ],
   "source": [
    "print(p_ab)\n",
    "print(p0_v)\n",
    "print(p1_v)\n",
    "print(my_vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as : 0\n",
      "['stupid', 'garbage'] classified as : 1\n"
     ]
    }
   ],
   "source": [
    "def classify_nb(vec2classify, p0_vec, p1_vec, p_class1):\n",
    "    \"\"\"\n",
    "    朴素贝叶斯分类函数\n",
    "    \n",
    "    Parameters\n",
    "        vec2classify：需要分类的词向量\n",
    "        p0_vec：每个词属于类别0（正常言论）的概率\n",
    "        p1_vect：每个词属于类别1（侮辱性言论）的概率\n",
    "        p_class1：文档属于侮辱性文档的概率\n",
    "    \"\"\"\n",
    "    p1 = sum(vec2classify * p1_vec) + log(p_class1)  # log(ab) = log(a) + log(b)\n",
    "    p0 = sum(vec2classify * p0_vec) + log(1.0 - p_class1)\n",
    "\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "test_list = [\n",
    "    ['love', 'my', 'dalmation'],\n",
    "    ['stupid', 'garbage']\n",
    "]\n",
    "\n",
    "for test_doc in test_list:\n",
    "    test_class = classify_nb(set_of_words2vec(my_vocab_list, test_doc), p0_v, p1_v, p_ab)\n",
    "    print(\"%s classified as : %s\" % (test_doc, test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the error rate is:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "def text_parse(big_string):\n",
    "    list_of_tokens = re.split(r'\\W*', big_string)\n",
    "    return [tok.lower() for tok in list_of_tokens if len(tok) > 2]\n",
    "\n",
    "def spam_test():\n",
    "    doc_list = []; class_list = []; full_text = []\n",
    "    for i in range(1,26):\n",
    "        word_list = text_parse(open('data/email/spam/%s.txt' % i,encoding='utf-8').read())\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(1)\n",
    "        word_list = text_parse(open('data/email/ham/%d.txt' % i,encoding='utf-8').read())\n",
    "        doc_list.append(word_list)\n",
    "        full_text.extend(word_list)\n",
    "        class_list.append(0)\n",
    "    \n",
    "    vocab_list = create_vocab_list(doc_list)\n",
    "    training_set = list(range(50)); test_set = []\n",
    "    for i in range(10):\n",
    "        rand_index = int(np.random.uniform(0, len(training_set)))\n",
    "        test_set.append(training_set[rand_index])\n",
    "        del training_set[rand_index]\n",
    "\n",
    "    train_mat = []; train_classes = []\n",
    "    for doc_index in training_set:\n",
    "        train_mat.append(set_of_words2vec(vocab_list, doc_list[doc_index]))\n",
    "        train_classes.append(class_list[doc_index])\n",
    "\n",
    "    p0_v, p1_v, p_spam = train_nb0(np.array(train_mat), np.array(train_classes))\n",
    "\n",
    "    error_count = 0\n",
    "\n",
    "    for doc_index in test_set:\n",
    "        word_vector = set_of_words2vec(vocab_list, doc_list[doc_index])\n",
    "        if classify_nb(np.array(word_vector), p0_v, p1_v, p_spam) != class_list[doc_index]:\n",
    "            error_count += 1\n",
    "    print(\"the error rate is: \", float(error_count) / len(test_set))\n",
    "\n",
    "spam_test()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
